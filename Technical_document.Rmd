---
title: "Predictive Analysis on Premium Subscriber Conversion in Social Networking Streaming Industry"
author: "Yvonne Chang"
date: "2023-07-22"
output:
  word_document: default
  html_document: default
---

Please note that some description parts are adjusted after rendering. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
```

# GROUP PART -- technical document
Table of contents:
1. Things to Notice before Analysis
1. Problem defining and our overall rationale to solve it
2. EDA before Prediction:
  (1) due to the imbalanced dataset, oversampling to train the model is needed
  (2) not very strong positive correlations exist; we will fit the model later to check whether there are severe multicollinearity using Variance Inflation Factor (VIF)
3. Normalization: min-max normalization
4. Before Feature Selection: with / without oversampling + no feature selection
5. Feature Selection: Filter Approach
6. Model Fitting and Performance Evaluation:
  (1) fit the models with the filtered data set
  (2) 10-Folds Cross-Validation with oversampled training set within each folds
  (2) select the final model based on AUC
  (3) generate dashboard of threshold and their corresponding precision and recall_p
7. Summary and other analysis results
8. Translate analyzing results into business solutions outline
9. Appendix: more model tuning and selection

## Things to Notice before Analysis

### Don't use Accuracy in Imbalanced dataset; Use AUC, Precision and Recall_p
From the EDA later we will know that, the proportion of the 2 levels in response variable is severely imbalanced, indicating we shouldn't use accuracy as our evaluation standard since it will conclude incorrect results.

Thus, our priority of model selection is based on AUC, then precision and recall_p.
However, we are going to be not so restricted: if there's no really big difference in those metrices, we prefer more explainable model in business point of view.

### Understanding the scope of our analysis: prediction task, not causal analysis
Note that all the data are originally non-subscribers.
It is essential to emphasize our goal for understanding which people would be likely to convert from free users to premium subscribers in the next 6 month period if they are targeted by our promotion campaign.
We care about correlation and can't say "they turn into premium users DUE TO our promotion" since it's NOT an causal problem which statistical method we have now cannot solve.

So please notice the description here: we are NOT going to use CAUSE, DUE TO...


## Problem defining and our overall rationale to solve it

### General goal for analysis
To get a deeper understanding of which people would be likely to convert from free users to premium subscribers in the next 6 month period if they are targeted by our promotion campaign.

### Specific goal for analysis: 
1. train model to conduct prediction task, then select the final one based on AUC, precision, and recall_p.
2. we need to do EDA before moving on to further analysis, and we are going to combine analysis from that as well as model selection to provide data support for business solutions.
3. we will translate analyzing results into business solutions outline, and more detailed business strategies will be presented in our managerial document. 


## EDA before Prediction
```{r}
setwd('C:/Users/Yvonne/Desktop/UMN Courses/6131')
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(ROSE) 

xyzdata <- read.csv('XYZData.csv')
```

Check missing values: no missing values
```{r}
sum(is.na(xyzdata == TRUE)) # no missing values
```

Check class proportion: imbalanced data
```{r}
# pie chart for the response variable
pie(table(xyzdata$adopter), labels = round(table(xyzdata$adopter)/41540, 2), main = "Adopter Proportion Pie Chart", col = rainbow(2))
legend("topright", c("0: no","1: yes"), cex = 0.8, fill = rainbow(2))
```
Due to the severe imbalanced data, oversampling to train the model is needed.

Check the correlation among variables to get a conceptual understanding of the dataset.
```{r}
library(corrplot)
corrplot(cor(xyzdata[, 2:26]), type = 'upper', addCoef.col = 'brown', tl.cex = 0.5, number.cex = 0.3)
```
No significant negative correlation but some positive correlations we might want to notice later. 

The followings are some reasons we consider why that positive correlation happened:
1. age / avg_friend_age: people generally tends to have friends with similar age range.
2. friend_cnt / friend_country_cnt: a person with more friends tends to have more friends from more different countries 
3. friend_cnt / subscriber_friend_cnt: a person with more friends tends to have more friends that are subscribers.

Check relationship between response and each predictor.
```{r}
par(mfrow = c(2, 2))
plot(xyzdata$age, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$male, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$friend_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$avg_friend_age, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(2, 2))
plot(xyzdata$avg_friend_male, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$friend_country_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$subscriber_friend_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$songsListened, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(2, 2))
plot(xyzdata$lovedTracks, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$posts, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$playlists, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$shouts, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(2, 2))
plot(xyzdata$delta_friend_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_avg_friend_age, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_avg_friend_male, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_friend_country_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(2, 2))
plot(xyzdata$delta_subscriber_friend_cnt, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_songsListened, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_lovedTracks, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_posts, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(2, 2))
plot(xyzdata$delta_playlists, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$delta_shouts, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$tenure, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)
plot(xyzdata$good_country, xyzdata$adopter, type = "p", pch = 20, cex = 0.5)

par(mfrow = c(1, 2))
pie(table(xyzdata$male), labels = round(table(xyzdata$male)/41540, 2), main = "Gender Proportion Pie Chart", col = rainbow(2))
legend("topleft", c("0: female","1: male"), cex = 0.8, fill = rainbow(2))

pie(table(xyzdata$good_country), labels = round(table(xyzdata$good_country)/41540, 2), main = "Good Country Proportion Pie Chart", col = rainbow(2))
legend("topright", c("0: more limited","1: less limited"), cex = 0.8, fill = rainbow(2))

pie(table(xyzdata$delta_good_country), labels = round(table(xyzdata$delta_good_country)/41540, 2), main = "Delta Good Country Proportion Pie Chart", col = rainbow(3))
legend("topright", c("-1: become more limited", "0: unchanged", "1: become less limited"), cex = 0.8, fill = rainbow(3))
```
Variables seems to have pattern, and please note that this is a little bit subjective judgement:
(>>>>> means we consider the variable acts(or ranges) more differently in adopter_1 and adopter_0 compared to others, i.e. more possible patterns)
delta_shouts >>>>>
delta_playlist
delta_posts >>>
delta_lovedTracks >>>>>
delta_songListened >>>>>
delta_subscriber_friend_cnt
delta_avg_friend_age >>>>>
delta_avg_friend_male >>>
delta_friend_cnt >>>>>
posts
lovedTracks >>>>>
shouts >>>>>
playlists
songListened >>>>>
subscriber_friend_cnt
friend_country_cnt
avg_friend_age
friend_cnt

And we have more limited countries without changing status and more males than females.

Another thing we want to mention is that, we guess the performance of model fitting might not be so ideal since we can't observe clear pattern in EDA, so we might focus on finding a model that can get the most senses in business perspective rather than "torchering" data to get as much as information we want. We will definitely do adjustment to find better one, we are saying that although numerically thinking the performance might not be perfect, yet how we can apply that in business strategy is more important.


## Normalization
Implement min-max normalization before predicting.
```{r}
normalize = function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# And transfer the response into factors for the two datasets.
xyzdata$adopter <- as.factor(xyzdata$adopter)

# use the mutate_at() to specify the indexes of columns needed normalization
# we can and need to normalize both binary and numerical data, except for adopter and user_id
xyzdata_normalized <- xyzdata %>% mutate_at(c(2:26), normalize)

# we drop the user_id since it' just an index
xyzdata_normalized_drop_user_id <- xyzdata_normalized[, -1]

# use createDataPartition() fo split the training and testing dataset for w. delta data
train_rows <- createDataPartition(y = xyzdata_normalized_drop_user_id$adopter, p = 0.75, list = FALSE)
xyzdata_normalized_drop_user_id_train <- xyzdata_normalized_drop_user_id[train_rows, ]
xyzdata_normalized_drop_user_id_test <- xyzdata_normalized_drop_user_id[-train_rows, ]
```


## Before Feature Selection: with / without oversampling + no feature selection

### Logistic regression: no oversampling, no feature selection
```{r}
fit_log <- glm(adopter ~ ., data = xyzdata_normalized_drop_user_id_train, family = binomial)
summary(fit_log)
```
Prediction
```{r}
pred_fit_log <- predict(fit_log, newdata = xyzdata_normalized_drop_user_id_test, type = "response")
```
Check the accuracy by measuring AUC
```{r}
roc_log <- roc.curve(xyzdata_normalized_drop_user_id_test$adopter, pred_fit_log, col = "blue", lwd = 2)
roc_log$auc
```
Generate a dataframe of cutoff and corresponding recall_p and precision.
```{r}
# initialize the dataframe 
dashboard <- data.frame()

# initialize vectors
cutoff <- c()
precision <- c()
recall_p <- c()

# for loop to get corresponding recall_p and precision for each cutoff value
threshold <- roc_log$thresholds
for (i in 1:(length(threshold))){
  cutoff <- c(cutoff, threshold[i])
  binary_predictions <- ifelse(pred_fit_log >= threshold[i], 1, 0)
  confusion_matrix <- confusionMatrix(data = factor(binary_predictions), reference = xyzdata_normalized_drop_user_id_test$adopter, mode = "prec_recall", positive = "1")
  recall_p <- c(recall_p, roc_log$true.positive.rate[i])
  precision <- c(precision, confusion_matrix$byClass[["Precision"]]) 
}

dashboard <- data.frame(cutoff, recall_p, precision)
dashboard
```


### Cross validation for logistics regression, oversampling, no feature selection
We use 10 folds cross-validation.
Note that if we want to combind cross-validation and oversampling, we should oversample the 9 folds as training each time INSIDE the loop.
```{r}
library(caret)

# create a list of row indexes that correspond to each folds
cv <- createFolds(y = xyzdata_normalized_drop_user_id$adopter, k = 10) 

# a vector to store auc from each fold
AUC_cv <- c()

for(test_rows in cv){
  xyz_train <- xyzdata_normalized_drop_user_id[-test_rows, ]
  xyz_test <- xyzdata_normalized_drop_user_id[test_rows, ]
  
  # oversample the training set
  library(ROSE)
  xyz_train_oversample_cv <- ROSE(adopter ~., data = xyz_train, seed = 123)$data
  
  # train the model then evaluate its performance
  fit_log_cv <- glm(adopter ~ ., data = xyz_train_oversample_cv, family = binomial)
  
  # predict
  pred_fit_log_cv <- predict(fit_log_cv, newdata = xyz_test, type = "response")
  
  # get auc
  roc_cv <- roc(xyz_test$adopter, pred_fit_log_cv, col = "blue", lwd = 2)
  auc_cv <- roc_cv$auc
  
  # add auc of current folds
  AUC_cv <- c(AUC_cv, auc_cv)
}

# report average accuracy across folds
mean(AUC_cv)
```
Oversampling training set helps in performance

Check multicollinearity
```{r}
library(car)
vif(fit_log_cv)
```
Variance Inflation Factors: VIF = 1/(1 - R_squared^2), detects multicollinearity in regression analysis. 
Multicollinearity happens when independent variables in a regression model are highly correlated to each other, making it hard to interpret the model and also causes problems in performance.

Reading VIF:
VIF of 1.9 indicates the variance of a particular coefficient is 90% higher than what we would expect if there was no multicollinearity, i.e. the variance of a particular coefficient is 90% higher than being orthogonal.

Usually VIF < 2 is not going to cause problems, which is the case here. 
But we still want to do PCA later since too many variables makes the model hard to interpret and some variables might measure similar factors.


## Feature Selection: Filter Approach

### Filtering

```{r}
library(FSelectorRcpp)
IG <- information_gain(adopter ~ ., data = xyzdata_normalized_drop_user_id)

# select top 10
top10 <- cut_attrs(IG, k = 10)

# the whole normalized dataset
xyzdata_normalized_drop_user_id_top10 <- xyzdata_normalized_drop_user_id %>% select(top10, adopter)

# training set
xyzdata_normalized_drop_user_id_train_top10 <- xyzdata_normalized_drop_user_id_train %>% select(top10, adopter)

# testing set
xyzdata_normalized_drop_user_id_test_top10 <- xyzdata_normalized_drop_user_id_test %>% select(top10, adopter)
```
lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male

### Cross validation for logistics regression, oversampling, filtered 
We use 10 folds cross-validation
```{r}
library(caret)

# create a list of row indexes that correspond to each folds
cv <- createFolds(y = xyzdata_normalized_drop_user_id_top10$adopter, k = 10)

# a vector to store auc from each fold
AUC_cv_filter <- c()

for(test_rows in cv){
  xyz_train_f <- xyzdata_normalized_drop_user_id_top10[-test_rows, ]
  xyz_test_f <- xyzdata_normalized_drop_user_id_top10[test_rows, ]
  
  # oversample the training folds
  xyz_train_oversample_filter <- ROSE(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, data = xyz_train_f, seed = 123)$data
  
  # train the model then evaluate its performance
  fit_log_oversample_filter_cv <- glm(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, data = xyz_train_oversample_filter, family = binomial)
  
  # predict
  pred_fit_log_ftiler_cv <- predict(fit_log_oversample_filter_cv, newdata = xyz_test_f, type = "response")
  
  # get auc
  roc_cv_filter <- roc.curve(xyz_test_f$adopter, pred_fit_log_ftiler_cv)
  auc_cv_filter <- roc_cv_filter$auc
  
  # add auc of current folds
  AUC_cv_filter <- c(AUC_cv_filter, auc_cv_filter)
}

# report average accuracy across folds
mean(AUC_cv_filter) 
```

Check multicollinearity
```{r}
vif(fit_log_oversample_filter_cv)
```
So far, we've seen some important points:
1. oversampling is needed.
2. no severe multicollinearity problems needed to concern, but we will still try PCA later.
3. filtered as top 10 variables act in relatively clear pattern in EDA previously. 

Generate a dataframe of cutoff and corresponding recall_p and precision.
```{r}
# initialize the dataframe 
dashboard_filter_log <- data.frame()

# initialize vectors
cutoff_filter_log <- c()
precision_filter_log <- c()
recall_p_filter_log <- c()

# for loop to get corresponding recall_p and precision for each cutoff value
threshold <- roc_cv_filter$thresholds
for (i in 1:(length(threshold))){
  cutoff_filter_log <- c(cutoff_filter_log, threshold[i])
  binary_predictions <- ifelse(pred_fit_log_ftiler_cv >= threshold[i], 1, 0)
  confusion_matrix <- confusionMatrix(data = factor(binary_predictions), reference = xyz_test_f$adopter, mode = "prec_recall", positive = "1")
  recall_p_filter_log <- c(recall_p_filter_log, roc_cv_filter$true.positive.rate[i])
  precision_filter_log <- c(precision_filter_log, confusion_matrix$byClass[["Precision"]]) 
}

dashboard_filter_log <- data.frame(cutoff_filter_log, recall_p_filter_log, precision_filter_log)
dashboard_filter_log
```


## PCA: oversampling, logistic regression, only transform relatively highly correlated variables

```{r}
# we use oversampled training set
library(ROSE)
xyz_train_oversample_pca <- ROSE(adopter ~., data = xyzdata_normalized_drop_user_id_train, seed = 123)$data

# get PCs
high_cor <- xyz_train_oversample_pca[, c(1, 3, 4, 6, 7, 8, 9, 12, 13, 16, 18, 19, 22)]
xyzdata_rose_eigen_high_cor <- eigen(cor(high_cor))
xyzdata_rose_pca_high_cor <- prcomp(high_cor)
summary(xyzdata_rose_pca_high_cor)

# colnames(xyz_train_oversample_pca[, c(1, 3, 4, 6, 7, 8, 9, 12, 13, 16, 18, 19, 22)])

screeplot(xyzdata_rose_pca_high_cor, type = "lines")
```
From the screeplot and summary of PCs, it seems the first 6 PCs are more important since the cumulative proportion of variance explained by them is about 95.9% for those variables with higher correlations.

We also check relationships of PCs and those variables with higher correlations.
```{r}
xyzdata_rose_pca_high_cor$rotation
```
(will be replaced with the labeled picture PDF file after rendering.)

```{r}
xyzdata_rose_pca_high_cor_score <- xyzdata_rose_pca_high_cor$x

# plot the relationship 
par(mfrow = c(1, 2))
plot(xyzdata_rose_pca_high_cor_score[, "PC1"], high_cor$age, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC1"], high_cor$avg_friend_age, pch = 20, cex = 0.8)
par(mfrow = c(1, 1))
plot(xyzdata_rose_pca_high_cor_score[, "PC2"], high_cor$friend_country_cnt, pch = 20, cex = 0.8)
par(mfrow = c(1, 2))
plot(xyzdata_rose_pca_high_cor_score[, "PC3"], high_cor$age, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC3"], high_cor$avg_friend_age, pch = 20, cex = 0.8)
par(mfrow = c(1, 2))
plot(xyzdata_rose_pca_high_cor_score[, "PC4"], high_cor$songsListened, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC4"], high_cor$lovedTracks, pch = 20, cex = 0.8)
par(mfrow = c(1, 2))
plot(xyzdata_rose_pca_high_cor_score[, "PC5"], high_cor$songsListened, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC5"], high_cor$lovedTracks, pch = 20, cex = 0.8)
par(mfrow = c(2, 2))
plot(xyzdata_rose_pca_high_cor_score[, "PC6"], high_cor$delta_friend_cnt, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC6"], high_cor$delta_friend_country_cnt, pch = 20, cex = 0.8)
plot(xyzdata_rose_pca_high_cor_score[, "PC6"], high_cor$delta_shouts, pch = 20, cex = 0.8)

# identify the PCs, give up PC3
age_related <- xyzdata_rose_pca_high_cor_score[, "PC1"]
friend_diversity <- -1*xyzdata_rose_pca_high_cor_score[, "PC2"]
tend_to_exploration <- xyzdata_rose_pca_high_cor_score[, "PC4"]
tend_to_concentration <- xyzdata_rose_pca_high_cor_score[, "PC5"]
information_received <- xyzdata_rose_pca_high_cor_score[, "PC6"]

# fit the logistic regression
PC1 <- scale(age_related)
PC2 <- scale(friend_diversity)
PC4 <- scale(tend_to_exploration)
PC5 <- scale(tend_to_concentration)
PC6 <- scale(information_received)
```
Also consider the plots above: 
Until now, we decide to extract the first 6 PCs except for the 3rd since the variable it measures is already in PC1 and its magnitude is too small and important information may already be included in PC1.
PC1: capture age_related information, maybe usage and age, including age, avg_friend_age.
PC2: capture friend_diversity information, including friend_country_cnt.
PC4: capture tend_to_exploration information, meaning that users might have many already loved content and also tend to try new things, including songsListened, lovedTracks
PC5: capture tend_to_concentration information, meaning that users might have many already loved content but not tend to try new things, including songsListened, lovedTracks
PC6: capture information_received ability since delta_shots is the changed number of wall posts received, and increase in number of friends and friend's countries will also have effects on information_received ability, including delta_shouts, delta_friend_cnt, delta_friend_country_cnt.

Thus we identify the 5 PCs: 
PC1(age_related), PC2(friend_diversity), PC4(tend_to_exploration), PC5(tend_to_concentration), PC6(information_received).

Prepare PCA training and testing set for PCA model.
```{r}
# remained
remained_train <- xyz_train_oversample_pca[, c(-1, -3, -4, -6, -7, -8, -9, -12, -13, -16, -18, -19, -22)]
remained_test <- xyzdata_normalized_drop_user_id_test[, c(-1, -3, -4, -6, -7, -8, -9, -12, -13, -16, -18, -19, -22)]

# prepare PCA training set
train_data_pca <- predict(xyzdata_rose_pca_high_cor, xyz_train_oversample_pca[, -which(names(xyzdata_normalized_drop_user_id_test) == "adopter")])
train_data_pca_6pc <- train_data_pca[, c(1:2, 4:6)] #give up pc3
train_data_pca_6pcAndremained <- cbind(remained_train, train_data_pca_6pc)

# prepare PCA testing set
test_data_pca <- predict(xyzdata_rose_pca_high_cor, xyzdata_normalized_drop_user_id_test[, -which(names(xyzdata_normalized_drop_user_id_test) == "adopter")])
test_data_pca_6pc <- test_data_pca[, c(1:2, 4:6)] #give up pc3
test_data_pca_6pcAndremained <- cbind(remained_test, test_data_pca_6pc)

# fit the model
fit_PCA <- glm(adopter ~ PC1 + PC2 + PC4 + PC5 + PC6 + male + avg_friend_male + posts + playlists + delta_avg_friend_age + delta_avg_friend_male + delta_subscriber_friend_cnt + delta_posts + delta_playlists + tenure + good_country + delta_good_country, data = train_data_pca_6pcAndremained, family = binomial)

# check variance inflation factors
library(car)
vif(fit_log_cv) # logistics regression, oversampling, no feature selection
vif(fit_PCA)
```
Check VIFs of the logistics regression, oversampling, no feature selection again, they are not in a big problem of multicollinearity, but after fitting PCs in the model, they become lower, which is a good thing.

And if we check again summary of (logistics regression, oversampling, no feature selection) and (logistic regression, oversampling, PCA for transforming relatively highly correlated variables):
```{r}
summary(fit_log)
summary(fit_PCA)
```
There become less insignificant variables in the latter and we also reduce dimension, making the model more easily to explain. So so far we prefer PCA logistic regression model than logistic regression without feature selection.

Prediction
```{r}
# predict
pred_fit_PCA <- predict(fit_PCA, newdata = test_data_pca_6pcAndremained, type = "response")

# auc
roc_pca <- roc.curve(test_data_pca_6pcAndremained$adopter, pred_fit_PCA, , col = "blue", lwd = 2)
auc_pca <- roc_pca$auc
roc_pca
```
If AUC not changing a lot, a more easily explainable model is preferred. 

Generate a dataframe of cutoff and corresponding recall_p and precision.
```{r}
# initialize the dataframe 
dashboard_pca <- data.frame()

# initialize vectors
cutoff_pca <- c()
precision_pca <- c()
recall_p_pca <- c()

# for loop to get corresponding recall_p and precision for each cutoff value
threshold <- roc_pca$thresholds
for (i in 1:(length(threshold))){
  cutoff_pca <- c(cutoff_pca, threshold[i])
  binary_predictions <- ifelse(pred_fit_PCA >= threshold[i], 1, 0)
  confusion_matrix <- confusionMatrix(data = factor(binary_predictions), reference = test_data_pca_6pcAndremained$adopter, mode = "prec_recall", positive = "1")
  recall_p_pca <- c(recall_p_pca, roc_pca$true.positive.rate[i])
  precision_pca <- c(precision_pca, confusion_matrix$byClass[["Precision"]]) 
}

dashboard_pca <- data.frame(cutoff_pca, recall_p_pca, precision_pca)
dashboard_pca
```


## More Model Fitting and Performance Evaluation: naive Bayes model, filtered

We use the filtered dataset we got previously.
```{r}
library(e1071)

# oversample the training set
xyzdata_normalized_drop_user_id_train_top10_oversampled <- ROSE(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, data = xyzdata_normalized_drop_user_id_train_top10, seed = 123)$data

# train the model
NB_model_oversample <- naiveBayes(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, data = xyzdata_normalized_drop_user_id_train_top10_oversampled)

# predict
preb_prob_nb_oversample <- predict(NB_model_oversample, xyzdata_normalized_drop_user_id_test_top10, type = "raw") 
# class probability predictions by setting type = "raw"
  
# get auc
library(pROC)

xyzdata_normalized_test_roc_curve_NB <- xyzdata_normalized_drop_user_id_test_top10 %>% mutate(prob = preb_prob_nb_oversample[, "1"]) %>% arrange(desc(prob)) %>% mutate(yes_1 = ifelse(adopter == "1", 1, 0)) 

roc_nb <- roc.curve(xyzdata_normalized_test_roc_curve_NB$yes_1, xyzdata_normalized_test_roc_curve_NB$prob)
auc_nb <- roc_nb$auc
auc_nb
```

Generate a dataframe of cutoff and corresponding recall_p and precision.
```{r}
# initialize the dataframe 
dashboard_filter <- data.frame()

# initialize vectors
cutoff_filter <- c()
precision_filter <- c()
recall_p_filter <- c()

# for loop to get corresponding recall_p and precision for each cutoff value
threshold <- roc_nb$thresholds
for (i in 1:(length(threshold))){
  cutoff_filter <- c(cutoff_filter, threshold[i])
  binary_predictions <- ifelse(xyzdata_normalized_test_roc_curve_NB$prob >= threshold[i], 1, 0)
  confusion_matrix <- confusionMatrix(data = factor(binary_predictions), reference = xyzdata_normalized_drop_user_id_test_top10$adopter, mode = "prec_recall", positive = "1")
  recall_p_filter <- c(recall_p_filter, roc_nb$true.positive.rate[i])
  precision_filter <- c(precision_filter, confusion_matrix$byClass[["Precision"]]) 
}

dashboard_filter <- data.frame(cutoff_filter, recall_p_filter, precision_filter)
dashboard_filter
```
Precision and recall_p is not performed good in naive Bayes model.


## Summary and Suggestions for Business solutions outline

From the analysis above, what we will suggest for business strategy is that, pick the model that is more explainable and feasible in business scope rather than only focusing in numerical value of performance. 


## Appendix: more model tuning and selection 

### Decision tree, filtered on oversampled training set
```{r}
# fit the model
# split = "information" means we want to determine splits based on information gain
library(rpart)
tree <- rpart(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, data = xyzdata_normalized_drop_user_id_train_top10_oversampled, method = "class", parms = list(split = "information"), control = list(minsplit = 2, maxdepth = 500, cp = 0.0005))

library(rpart.plot)
prp(tree, varlen = 0)

pred <- predict(tree, xyzdata_normalized_drop_user_id_test_top10, type = "class")
confusion_matrix_tree <- confusionMatrix(data = factor(pred), reference = factor(xyzdata_normalized_drop_user_id_test_top10$adopter), mode = "prec_recall", positive = "1")

confusion_matrix_tree
```
Not really good performance, Precision and recall for tree model are too low.
Precision : 0.12093         
Recall : 0.29870  

```{r}
# auc
xyzdata_normalized_roc_curve_tree <- roc.curve(xyzdata_normalized_drop_user_id_test_top10$adopter, pred)
# plot(xyzdata_normalized_roc_curve_knn)
xyzdata_normalized_roc_curve_tree$auc
```


### Knn, filtered on oversampled training set
```{r}
# fit the model
library(kknn)

model_knn <- kknn(adopter ~ lovedTracks + delta_songsListened + delta_lovedTracks + subscriber_friend_cnt + songsListened + friend_cnt + friend_country_cnt + delta_friend_cnt + delta_subscriber_friend_cnt + delta_avg_friend_male, train = xyzdata_normalized_drop_user_id_train_top10_oversampled, test = xyzdata_normalized_drop_user_id_test_top10, k = 500, distance = 2, kernel = "rectangular")
pred_prob_knn <- model_knn$prob

# auc
xyzdata_normalized_roc_curve_knn <- roc.curve(ifelse(xyzdata_normalized_drop_user_id_test_top10$adopter == '1', 1, 0), pred_prob_knn[, "1"])
# plot(xyzdata_normalized_roc_curve_knn)
xyzdata_normalized_roc_curve_knn$auc
```
Not bad AUC. But if checking recall_p and precision, ...

```{r}
confusion_matrix_knn <- confusionMatrix(data = factor(ifelse(pred_prob_knn[, "1"] > 0.2, 1, 0)), reference = xyzdata_normalized_drop_user_id_test_top10$adopter, mode = "prec_recall", positive = "1")

confusion_matrix_knn
```
Precision : 0.08541         
Recall : 0.47273    
Not performing well even if we've tune the threshold low.


### Oversampling before Filtering
Since the data set is severely imbalanced, we oversample the whole training dataset, hoping the filtering procedure could capture more information.
```{r}
# oversample the whole training dataset
library(ROSE)
xyzdata_rose_whole_train <- ROSE(adopter ~., data = xyzdata_normalized_drop_user_id_train, seed = 123)$data

# xyzdata_normalized_drop_user_id_test: untouched

# filtering using oversampled training set
library(FSelectorRcpp)
IG_oversample_whole_train <- information_gain(adopter ~ ., data = xyzdata_rose_whole_train)

# select top 10
top10_oversampled <- cut_attrs(IG_oversample_whole_train, k = 10)

# oversampled training set
xyzdata_normalized_top_10_oversampled_train <- xyzdata_rose_whole_train %>% select(top10_oversampled, adopter)

# untouched testing, no oversampled
xyzdata_normalized_drop_user_id_test_top_10 <- xyzdata_normalized_drop_user_id_test %>% select(top10_oversampled, adopter)

# the whole dataset, no oversampled
xyzdata_normalized_drop_user_id_top_10 <- xyzdata_normalized_drop_user_id %>% select(top10_oversampled, adopter)

colnames(xyzdata_normalized_drop_user_id_top_10) 
```
Yet then we realized it's not stable since each time the top 10 variables are not exactly the same.
So we decided not to use it in the end.
